{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Device configuration\n",
    "device=torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "image_size = 784\n",
    "h_dim = 400\n",
    "z_dim = 20\n",
    "num_epochs = 20\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST dataset\n",
    "dataset = torchvision.datasets.MNIST(root='/data', train=False, transform=transforms.ToTensor())\n",
    "\n",
    "#Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size,  shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(image_size, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc3 = nn.Linear(h_dim, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, h_dim)\n",
    "        self.fc5 = nn.Linear(h_dim, image_size)\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = F.relu(self.fc1(x))\n",
    "        return self.fc2(h), self.fc3(h)\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(log_var/2)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        h = F.relu(self.fc4(z))\n",
    "        return F.sigmoid(self.fc5(h))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        x_reconst = self.decode(z)\n",
    "        return x_reconst, mu, log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a directory if not exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dir = 'generated'\n",
    "if not os.path.exists(sample_dir):\n",
    "    os.makedirs(sample_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ankit\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "C:\\Users\\Ankit\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/20], Step [10/79], Reconstruction Loss: 35030.8203, KL Divergence: 3198.4971\n",
      "Epoch[1/20], Step [20/79], Reconstruction Loss: 29060.1992, KL Divergence: 1023.0688\n",
      "Epoch[1/20], Step [30/79], Reconstruction Loss: 28055.2266, KL Divergence: 1231.1628\n",
      "Epoch[1/20], Step [40/79], Reconstruction Loss: 27444.1230, KL Divergence: 692.6370\n",
      "Epoch[1/20], Step [50/79], Reconstruction Loss: 26166.9492, KL Divergence: 659.7141\n",
      "Epoch[1/20], Step [60/79], Reconstruction Loss: 25527.2930, KL Divergence: 781.9853\n",
      "Epoch[1/20], Step [70/79], Reconstruction Loss: 24336.6777, KL Divergence: 1015.4268\n",
      "Epoch[2/20], Step [10/79], Reconstruction Loss: 22391.8047, KL Divergence: 1194.1521\n",
      "Epoch[2/20], Step [20/79], Reconstruction Loss: 22487.2285, KL Divergence: 1246.2677\n",
      "Epoch[2/20], Step [30/79], Reconstruction Loss: 21691.9707, KL Divergence: 1392.3047\n",
      "Epoch[2/20], Step [40/79], Reconstruction Loss: 20417.1562, KL Divergence: 1471.3330\n",
      "Epoch[2/20], Step [50/79], Reconstruction Loss: 20049.6680, KL Divergence: 1747.1360\n",
      "Epoch[2/20], Step [60/79], Reconstruction Loss: 19846.0059, KL Divergence: 1838.4158\n",
      "Epoch[2/20], Step [70/79], Reconstruction Loss: 19645.2949, KL Divergence: 1733.9924\n",
      "Epoch[3/20], Step [10/79], Reconstruction Loss: 18813.5723, KL Divergence: 1932.4156\n",
      "Epoch[3/20], Step [20/79], Reconstruction Loss: 18413.9082, KL Divergence: 1780.1667\n",
      "Epoch[3/20], Step [30/79], Reconstruction Loss: 17907.5508, KL Divergence: 2029.4873\n",
      "Epoch[3/20], Step [40/79], Reconstruction Loss: 17321.9531, KL Divergence: 1947.9485\n",
      "Epoch[3/20], Step [50/79], Reconstruction Loss: 17098.0312, KL Divergence: 1876.0380\n",
      "Epoch[3/20], Step [60/79], Reconstruction Loss: 17304.5039, KL Divergence: 2069.6692\n",
      "Epoch[3/20], Step [70/79], Reconstruction Loss: 16950.0117, KL Divergence: 2028.7239\n",
      "Epoch[4/20], Step [10/79], Reconstruction Loss: 16411.9766, KL Divergence: 2195.4321\n",
      "Epoch[4/20], Step [20/79], Reconstruction Loss: 16623.9414, KL Divergence: 2212.8845\n",
      "Epoch[4/20], Step [30/79], Reconstruction Loss: 16534.9492, KL Divergence: 2192.6411\n",
      "Epoch[4/20], Step [40/79], Reconstruction Loss: 15732.3975, KL Divergence: 2145.7651\n",
      "Epoch[4/20], Step [50/79], Reconstruction Loss: 14867.5869, KL Divergence: 2348.7913\n",
      "Epoch[4/20], Step [60/79], Reconstruction Loss: 15337.9199, KL Divergence: 2311.6880\n",
      "Epoch[4/20], Step [70/79], Reconstruction Loss: 15034.4297, KL Divergence: 2423.5322\n",
      "Epoch[5/20], Step [10/79], Reconstruction Loss: 14122.0508, KL Divergence: 2326.7878\n",
      "Epoch[5/20], Step [20/79], Reconstruction Loss: 14265.9434, KL Divergence: 2427.6572\n",
      "Epoch[5/20], Step [30/79], Reconstruction Loss: 15292.8633, KL Divergence: 2348.1064\n",
      "Epoch[5/20], Step [40/79], Reconstruction Loss: 13963.6562, KL Divergence: 2405.3760\n",
      "Epoch[5/20], Step [50/79], Reconstruction Loss: 14116.4219, KL Divergence: 2532.4333\n",
      "Epoch[5/20], Step [60/79], Reconstruction Loss: 14481.0645, KL Divergence: 2433.5740\n",
      "Epoch[5/20], Step [70/79], Reconstruction Loss: 14482.3164, KL Divergence: 2537.2324\n",
      "Epoch[6/20], Step [10/79], Reconstruction Loss: 14360.8545, KL Divergence: 2599.4204\n",
      "Epoch[6/20], Step [20/79], Reconstruction Loss: 14789.5254, KL Divergence: 2598.7690\n",
      "Epoch[6/20], Step [30/79], Reconstruction Loss: 14603.2100, KL Divergence: 2659.6472\n",
      "Epoch[6/20], Step [40/79], Reconstruction Loss: 13317.0840, KL Divergence: 2520.4360\n",
      "Epoch[6/20], Step [50/79], Reconstruction Loss: 13790.9062, KL Divergence: 2580.1570\n",
      "Epoch[6/20], Step [60/79], Reconstruction Loss: 14114.8672, KL Divergence: 2615.1750\n",
      "Epoch[6/20], Step [70/79], Reconstruction Loss: 13293.7402, KL Divergence: 2709.9087\n",
      "Epoch[7/20], Step [10/79], Reconstruction Loss: 13371.7314, KL Divergence: 2638.7788\n",
      "Epoch[7/20], Step [20/79], Reconstruction Loss: 13705.7812, KL Divergence: 2637.4966\n",
      "Epoch[7/20], Step [30/79], Reconstruction Loss: 13162.9395, KL Divergence: 2599.9531\n",
      "Epoch[7/20], Step [40/79], Reconstruction Loss: 12795.2676, KL Divergence: 2734.8687\n",
      "Epoch[7/20], Step [50/79], Reconstruction Loss: 12891.8398, KL Divergence: 2776.3770\n",
      "Epoch[7/20], Step [60/79], Reconstruction Loss: 12653.6895, KL Divergence: 2658.4507\n",
      "Epoch[7/20], Step [70/79], Reconstruction Loss: 12953.9805, KL Divergence: 2748.7742\n",
      "Epoch[8/20], Step [10/79], Reconstruction Loss: 12414.8496, KL Divergence: 2755.2158\n",
      "Epoch[8/20], Step [20/79], Reconstruction Loss: 13053.9990, KL Divergence: 2703.2681\n",
      "Epoch[8/20], Step [30/79], Reconstruction Loss: 12757.4277, KL Divergence: 2645.2373\n",
      "Epoch[8/20], Step [40/79], Reconstruction Loss: 12800.6094, KL Divergence: 2755.6328\n",
      "Epoch[8/20], Step [50/79], Reconstruction Loss: 12453.0127, KL Divergence: 2667.5430\n",
      "Epoch[8/20], Step [60/79], Reconstruction Loss: 13135.6406, KL Divergence: 2831.5435\n",
      "Epoch[8/20], Step [70/79], Reconstruction Loss: 13325.3906, KL Divergence: 2773.3545\n",
      "Epoch[9/20], Step [10/79], Reconstruction Loss: 13227.0605, KL Divergence: 2960.7996\n",
      "Epoch[9/20], Step [20/79], Reconstruction Loss: 12608.8594, KL Divergence: 2746.3672\n",
      "Epoch[9/20], Step [30/79], Reconstruction Loss: 12870.3232, KL Divergence: 2778.7905\n",
      "Epoch[9/20], Step [40/79], Reconstruction Loss: 12882.4531, KL Divergence: 2813.7803\n",
      "Epoch[9/20], Step [50/79], Reconstruction Loss: 12447.0400, KL Divergence: 2774.0493\n",
      "Epoch[9/20], Step [60/79], Reconstruction Loss: 12101.5977, KL Divergence: 2882.7258\n",
      "Epoch[9/20], Step [70/79], Reconstruction Loss: 12345.5137, KL Divergence: 2755.2300\n",
      "Epoch[10/20], Step [10/79], Reconstruction Loss: 11785.0010, KL Divergence: 2741.4104\n",
      "Epoch[10/20], Step [20/79], Reconstruction Loss: 11622.4199, KL Divergence: 2819.1904\n",
      "Epoch[10/20], Step [30/79], Reconstruction Loss: 12056.5000, KL Divergence: 2799.4375\n",
      "Epoch[10/20], Step [40/79], Reconstruction Loss: 12466.7656, KL Divergence: 2953.3970\n",
      "Epoch[10/20], Step [50/79], Reconstruction Loss: 12172.1797, KL Divergence: 2877.9082\n",
      "Epoch[10/20], Step [60/79], Reconstruction Loss: 12449.2969, KL Divergence: 2868.2427\n",
      "Epoch[10/20], Step [70/79], Reconstruction Loss: 11892.5254, KL Divergence: 2889.2334\n",
      "Epoch[11/20], Step [10/79], Reconstruction Loss: 12404.4355, KL Divergence: 3027.9556\n",
      "Epoch[11/20], Step [20/79], Reconstruction Loss: 11862.8184, KL Divergence: 2928.0310\n",
      "Epoch[11/20], Step [30/79], Reconstruction Loss: 11516.2627, KL Divergence: 2952.7146\n",
      "Epoch[11/20], Step [40/79], Reconstruction Loss: 11975.7285, KL Divergence: 2913.1831\n",
      "Epoch[11/20], Step [50/79], Reconstruction Loss: 12389.9375, KL Divergence: 2918.5122\n",
      "Epoch[11/20], Step [60/79], Reconstruction Loss: 12317.3711, KL Divergence: 2944.1792\n",
      "Epoch[11/20], Step [70/79], Reconstruction Loss: 12107.9844, KL Divergence: 3080.5601\n",
      "Epoch[12/20], Step [10/79], Reconstruction Loss: 11373.2529, KL Divergence: 2891.3928\n",
      "Epoch[12/20], Step [20/79], Reconstruction Loss: 11878.7891, KL Divergence: 3022.3420\n",
      "Epoch[12/20], Step [30/79], Reconstruction Loss: 11807.7324, KL Divergence: 2950.5015\n",
      "Epoch[12/20], Step [40/79], Reconstruction Loss: 11695.0938, KL Divergence: 2925.5723\n",
      "Epoch[12/20], Step [50/79], Reconstruction Loss: 11622.0840, KL Divergence: 3049.4932\n",
      "Epoch[12/20], Step [60/79], Reconstruction Loss: 11979.0283, KL Divergence: 2935.1797\n",
      "Epoch[12/20], Step [70/79], Reconstruction Loss: 11767.1211, KL Divergence: 2909.0771\n",
      "Epoch[13/20], Step [10/79], Reconstruction Loss: 11339.5811, KL Divergence: 2875.0532\n",
      "Epoch[13/20], Step [20/79], Reconstruction Loss: 11681.4932, KL Divergence: 2999.6077\n",
      "Epoch[13/20], Step [30/79], Reconstruction Loss: 11635.2695, KL Divergence: 2938.1499\n",
      "Epoch[13/20], Step [40/79], Reconstruction Loss: 11649.8965, KL Divergence: 3038.7307\n",
      "Epoch[13/20], Step [50/79], Reconstruction Loss: 11334.5771, KL Divergence: 2974.1982\n",
      "Epoch[13/20], Step [60/79], Reconstruction Loss: 10996.3691, KL Divergence: 2973.4546\n",
      "Epoch[13/20], Step [70/79], Reconstruction Loss: 11535.7949, KL Divergence: 2991.0195\n",
      "Epoch[14/20], Step [10/79], Reconstruction Loss: 11382.2324, KL Divergence: 3001.7336\n",
      "Epoch[14/20], Step [20/79], Reconstruction Loss: 11011.8652, KL Divergence: 2961.6797\n",
      "Epoch[14/20], Step [30/79], Reconstruction Loss: 11185.4951, KL Divergence: 3033.7568\n",
      "Epoch[14/20], Step [40/79], Reconstruction Loss: 11513.9551, KL Divergence: 3092.3215\n",
      "Epoch[14/20], Step [50/79], Reconstruction Loss: 11553.9043, KL Divergence: 2983.9895\n",
      "Epoch[14/20], Step [60/79], Reconstruction Loss: 11455.1787, KL Divergence: 3115.3416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[14/20], Step [70/79], Reconstruction Loss: 11148.6445, KL Divergence: 2912.7747\n",
      "Epoch[15/20], Step [10/79], Reconstruction Loss: 10723.8281, KL Divergence: 2995.0884\n",
      "Epoch[15/20], Step [20/79], Reconstruction Loss: 11851.5518, KL Divergence: 2959.7539\n",
      "Epoch[15/20], Step [30/79], Reconstruction Loss: 11223.9355, KL Divergence: 3135.8267\n",
      "Epoch[15/20], Step [40/79], Reconstruction Loss: 10999.1504, KL Divergence: 2961.6819\n",
      "Epoch[15/20], Step [50/79], Reconstruction Loss: 11471.6621, KL Divergence: 3142.6206\n",
      "Epoch[15/20], Step [60/79], Reconstruction Loss: 11029.0059, KL Divergence: 2997.8335\n",
      "Epoch[15/20], Step [70/79], Reconstruction Loss: 11279.1943, KL Divergence: 3055.9851\n",
      "Epoch[16/20], Step [10/79], Reconstruction Loss: 11344.7568, KL Divergence: 2986.1458\n",
      "Epoch[16/20], Step [20/79], Reconstruction Loss: 11140.2637, KL Divergence: 3057.3330\n",
      "Epoch[16/20], Step [30/79], Reconstruction Loss: 11032.2070, KL Divergence: 3197.2778\n",
      "Epoch[16/20], Step [40/79], Reconstruction Loss: 11376.9141, KL Divergence: 2961.8555\n",
      "Epoch[16/20], Step [50/79], Reconstruction Loss: 11001.0723, KL Divergence: 3074.0139\n",
      "Epoch[16/20], Step [60/79], Reconstruction Loss: 11129.7178, KL Divergence: 2970.9111\n",
      "Epoch[16/20], Step [70/79], Reconstruction Loss: 11131.2715, KL Divergence: 3091.6948\n",
      "Epoch[17/20], Step [10/79], Reconstruction Loss: 10957.2607, KL Divergence: 3161.6389\n",
      "Epoch[17/20], Step [20/79], Reconstruction Loss: 10990.2617, KL Divergence: 3138.3601\n",
      "Epoch[17/20], Step [30/79], Reconstruction Loss: 11200.0527, KL Divergence: 2922.5706\n",
      "Epoch[17/20], Step [40/79], Reconstruction Loss: 11309.0273, KL Divergence: 3133.3208\n",
      "Epoch[17/20], Step [50/79], Reconstruction Loss: 11428.1758, KL Divergence: 3096.3311\n",
      "Epoch[17/20], Step [60/79], Reconstruction Loss: 11016.7314, KL Divergence: 3083.3228\n",
      "Epoch[17/20], Step [70/79], Reconstruction Loss: 10966.5029, KL Divergence: 2997.8342\n",
      "Epoch[18/20], Step [10/79], Reconstruction Loss: 10381.8916, KL Divergence: 3078.4404\n",
      "Epoch[18/20], Step [20/79], Reconstruction Loss: 10746.3340, KL Divergence: 2938.4951\n",
      "Epoch[18/20], Step [30/79], Reconstruction Loss: 10716.7920, KL Divergence: 3068.0522\n",
      "Epoch[18/20], Step [40/79], Reconstruction Loss: 11250.9805, KL Divergence: 3074.8716\n",
      "Epoch[18/20], Step [50/79], Reconstruction Loss: 11393.5625, KL Divergence: 3075.7930\n",
      "Epoch[18/20], Step [60/79], Reconstruction Loss: 11629.7891, KL Divergence: 3211.8945\n",
      "Epoch[18/20], Step [70/79], Reconstruction Loss: 10856.3643, KL Divergence: 3048.9077\n",
      "Epoch[19/20], Step [10/79], Reconstruction Loss: 10818.9463, KL Divergence: 3055.5930\n",
      "Epoch[19/20], Step [20/79], Reconstruction Loss: 10788.3105, KL Divergence: 3054.8384\n",
      "Epoch[19/20], Step [30/79], Reconstruction Loss: 11172.8740, KL Divergence: 3021.1565\n",
      "Epoch[19/20], Step [40/79], Reconstruction Loss: 11005.2686, KL Divergence: 3033.5820\n",
      "Epoch[19/20], Step [50/79], Reconstruction Loss: 11050.0439, KL Divergence: 3180.0696\n",
      "Epoch[19/20], Step [60/79], Reconstruction Loss: 10804.6064, KL Divergence: 3021.0659\n",
      "Epoch[19/20], Step [70/79], Reconstruction Loss: 10803.1475, KL Divergence: 3159.4570\n",
      "Epoch[20/20], Step [10/79], Reconstruction Loss: 10671.2090, KL Divergence: 3054.0710\n",
      "Epoch[20/20], Step [20/79], Reconstruction Loss: 10623.7822, KL Divergence: 3123.8542\n",
      "Epoch[20/20], Step [30/79], Reconstruction Loss: 11028.8779, KL Divergence: 3156.5234\n",
      "Epoch[20/20], Step [40/79], Reconstruction Loss: 10576.7988, KL Divergence: 3084.3977\n",
      "Epoch[20/20], Step [50/79], Reconstruction Loss: 10724.4531, KL Divergence: 3128.2705\n",
      "Epoch[20/20], Step [60/79], Reconstruction Loss: 11175.1729, KL Divergence: 3162.1982\n",
      "Epoch[20/20], Step [70/79], Reconstruction Loss: 10338.1963, KL Divergence: 3037.9761\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (x, _) in enumerate(data_loader):\n",
    "        # Forward pass\n",
    "        x = x.to(device).view(-1, image_size)\n",
    "        x_reconst, mu, log_var = model(x)\n",
    "        \n",
    "        #Compute reconstruction loss and KL divergence\n",
    "        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n",
    "        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        #Backprop and optimize\n",
    "        loss = reconst_loss + kl_div\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print (\"Epoch[{}/{}], Step [{}/{}], Reconstruction Loss: {:.4f}, KL Divergence: {:.4f}\" \n",
    "                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n",
    "            \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            # Save the sampled images\n",
    "            z = torch.randn(batch_size, z_dim).to(device)\n",
    "            out = model.decode(z).view(-1, 1, 28, 28)\n",
    "            save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n",
    "\n",
    "            # Save the reconstructed images\n",
    "            out, _, _ = model(x)\n",
    "            x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n",
    "            save_image(x_concat, os.path.join(sample_dir, 'reconstruction-{}.png'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
